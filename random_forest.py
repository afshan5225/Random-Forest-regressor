# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eB2ckA1D2A57PYzGuUcO7RTI9RH3vgtP

Terminologies
- Bias - the error of the training data
- Varience - the error of the testing data
- underfitting (high bias , high varience)
- overfitting (low bias,high varience)

importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error

df= pd.read_csv("/content/drive/MyDrive/dataset/IceCreamData.csv")
df.head()

df.info()

X = np.array(df.Temperature)
y = np.array(df.Revenue)

"""Splitting the dataset

"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.05)

"""#model selection"""

model = RandomForestRegressor(n_estimators=10,random_state=0)

model.fit(X_train.reshape(-1,1),y_train.reshape(-1,1))

y_pred = model.predict(X_test.reshape(-1,1))

comp = pd.DataFrame({"Actual": y_test,"Predicted":y_pred})
comp.head()

plt.scatter(X_test,y_test,color ="red")
plt.scatter(X_test,y_pred,color= 'Green')
plt.xlabel("Temp")
plt.ylabel("Revenue")

sns.heatmap(data = comp.corr(),annot = True,cmap = 'Greens')

r2_score(y_test,y_pred)

"""#RANDOM FOREST REGRESSOR PROJECT"""

dfs = pd.read_csv('/content/drive/MyDrive/dataset/cardekho_data.csv')
dfs.head()

dfs.shape

dfs['Owner'].unique()

dfs['Fuel_Type'] = dfs['Fuel_Type'].map({'Petrol': 0,'Diesel': 1,'CNG':2})
dfs['Seller_Type'] = dfs['Seller_Type'].map({'Dealer': 0,'Individual': 1})
dfs['Transmission'] = dfs['Transmission'].map({'Manual': 0,'Automatic': 1})
dfs['no_of_years'] = 2021 - dfs['Year']
dfs.drop(columns = ['Car_Name','Year'],inplace =True)

data = dfs.copy()
data.head()

data.describe()

data.info()

#sns.pairplot(data)

#sns.heatmap(data = data.corr(),annot = True)

#X = np.array(data.drop('Selling_Price',axis = 1))
#y = np.array(data['Selling_Price'])

X = data.drop('Selling_Price',axis = 1)
y = data['Selling_Price']

"""#Feature selection"""

from sklearn.ensemble import ExtraTreesRegressor

model =ExtraTreesRegressor()

feat_imp = model.fit(X,y)

feat_imp.feature_importances_

imp = pd.Series(feat_imp.feature_importances_, index = X.columns)
imp

imp.nlargest(5).plot(kind ='bar')

"""#Splittind the data"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=0)

"""Model selection"""

model  =RandomForestRegressor()

"""Hyper parameter Tuning"""

n_estimators = [int(i) for i in np.linspace(start = 100,stop = 1200,num = 12)]
max_features = ['auto','sqrt']

max_depth =[int(i) for i in np.linspace(start = 5,stop = 30,num = 6)]
min_samples_split =[2,5,10,15,100]
min_samples_leaf =[1,2,5,10]

random_grid = {'n_estimators':n_estimators,
               'max_depth':max_depth,
               'min_samples_split':min_samples_split,
               'min_samples_leaf':min_samples_leaf,
               'max_features':max_features,
               'max_depth':max_depth




}
print(random_grid)

from sklearn.model_selection import RandomizedSearchCV

rf_reg = RandomizedSearchCV(estimator = model,param_distributions =random_grid,scoring ='neg_mean_squared_error',cv =5,random_state =42,n_jobs =1 )

"""Trainong the model
- checking which combination of the parameters gives the best accuracy
"""

rf_reg.fit(X_train,y_train)

rf_reg.best_params_ #checking the parameter

y_pred = rf_reg.predict(X_test)

plt.scatter(y_test,y_pred)

finaldf = pd.DataFrame({'Acutal':y_test,'Pred':y_pred})
finaldf

sns.heatmap(data = finaldf.corr(),annot =True)

"""Accuracy"""

r2_score(y_test,y_pred)